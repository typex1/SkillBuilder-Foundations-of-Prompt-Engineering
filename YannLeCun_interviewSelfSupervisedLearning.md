Insert punctuation and paragraphs in the following text. Highlight keywords that are related to machine learning.

what is self-supervised learning and why is it the dark matter of intelligence i'll start by the dark matter part uh there is obviously a kind of learning that humans and animals are are doing that we currently are not reproducing properly with machines with ai right so the most popular approaches to machine learning today are or paradigms i should say are supervised running and reinforcement learning and they are extremely inefficient supervised learning requires many samples for learning anything and reinforcement learning requires a ridiculously large number of trials and errors to for you know a system to run anything um and that's why we don't have self-driving cars that's a big leap from one to the other okay so that to solve difficult problems you have to have a lot of uh human annotation for supervised learning to work and to solve those difficult problems with reinforcement learning you have to have some way to maybe simulate that problem such that you can do that large scale kind of learning that reinforcement learning requires right so how is it that you know most teenagers can learn to drive a car in about 20 hours of practice whereas even with millions of hours of simulated practice a self-driving car can't actually learn to drive itself properly um and so obviously we're missing something right and it's quite obvious for a lot of people that you know the immediate response you get from many people is well you know humans use their background knowledge to learn faster and they're right now how was that background knowledge acquired and that's the big question so now you have to ask you know how do babies in the first few months of life learn how the world works mostly by observation because they can hardly act in the world and they learn an enormous amount of background knowledge about the world that may be the the basis of what we call common sense this type of learning is not learning a task it's not being reinforced for anything it's just observing the world and figuring out how it works building world models learning world models how do we do this and how do we reproduce this in in machines so cell supervision learning is you know one instance of one attempt at trying to reproduce this kind of learning okay so you're looking at just observation so not even the interacting part of a child it's just sitting there watching mom and dad walk around pick up stuff all of that that's that's what you mean by background knowledge perhaps not even watching mom and dad just you know watching the world go by just having eyes open or having eyes closed or the very act of opening and closing eyes that the world appears and disappears all that basic information and you're saying in in order to learn to drive like the reason humans are able to learn to drive quickly some faster than others is because of the background knowledge they were able to watch cars operate in the world in the many years leading up to it the physics of basics objects all that kind of stuff that's right i mean the basic physics of objects you don't even know you don't even need to know you know how a car works right because that you can learn fairly quickly i mean the example i use very often is you're driving next to a cliff and you know in advance because of your you know understanding of intuitive physics that if you turn the wheel to the right the car will veer to the right will run off the cliff fall off the cliff and nothing good will come out of this right um but if you are a sort of you know tabularized reinforcement learning system that doesn't have a model of the world you have to repeat falling off this cliff thousands of times before you figure out it's a bad idea and then a few more thousand times before you figure out how to not do it and then a few more million times before you figure out how to not do it in every situation you ever encounter so self-supervised learning still has to have some source of truth being told to it by somebody and it's so you have to figure out a way without human assistance or without significant amount of human assistance to get that truth from the world so the mystery there is um how much signal is there how much truth is there that the world gives you whether it's the human world like you watch youtube or something like that or it's the more natural world so how much signal is there so here's the trick there is way more signal in sort of a self-supervised setting than there is in either supervised or reinforcement setting and this is going to my you know analogy of the cake the you know low cake as someone has called it where when you try to figure out how much information you ask the machine to predict and how much feedback you give the machine at every trial in reinforcement learning you give the machine a single scaler you tell the machine you did good you did bad and you you and you only tell this to the machine once in a while when i say you it could be the the universe telling the machine right um but it's just one scalar so as a consequence there is you you cannot possibly learn something very complicated without many many many trials where you get many many feedbacks of this type supervised running you you give a few bits to the machine at every every sample let's say you're training a system on you know recognizing images on imagenet there is 1000 categories that a little less than 10 bits of information per sample but star supervisor here is the setting you ideally we don't know how to do this yet but ideally you would show a machine a segment of a video and then stop the video and ask me ask the machine to predict what's going to happen next and so you let the machine predict and then you let time go by and show the machine what actually happened and hope the machine will you know learn to do a better job at predicting next time around there's a huge amount of information you give the machine because it's an entire video clip of uh you know of the future after the video clip you fed it in the first place so both for language and for vision there's a subtle seemingly trivial construction but maybe that's representative of what is required to create intelligence which is filling the gap so the gaps it sounds dumb but can you it's it is possible you can solve all of intelligence in this way just for both language just give a sentence and continue it or give a sentence and there's a gap in it uh some words blanked out and you fill in what words go there for vision you give a sequence of images and predict what's going to happen next or you fill in what happened in between do you think it's possible that formulation alone as a signal for self-supervised learning can solve intelligence for vision and language i think that's our best shot at the moment um so whether this will take us all the way to you know human-level intelligence or something or just cat-level intelligence uh it's not clear but among all the possible approaches that people have proposed i think it's our best shot so i think this idea of uh an intelligent system filling in the blanks either you know predicting the future inferring the past filling in missing information uh you know i'm currently filling the blank of what is behind your head and what you what your head looks like and you know from from the back uh because i have you know basic knowledge about how humans are made and i don't know if you're gonna you know where you're gonna say which point you're gonna speak whether you're gonna move your head this way or that way which way you're gonna look but i know you're not gonna just de-materialize and reappear three meters uh down the hall uh you know because i know what's possible and what's impossible uh according to into the physics so you have a model of what's possible what's impossible and then you'd be very surprised if it happens and then you'll have to reconstruct your model right so that that's the model of the world it's what tells you you know what fills in the blanks so given your partial information about the state of the world given by your perception uh your your model of the world fills in the missing information and that includes predicting the future retrodicting the past uh you know filling in things you don't immediately perceive and that doesn't have to be purely generic vision or visual information or generic language you can go to specifics like uh predicting what control decision you make when you're driving in a lane you have a sequence of images from a vehicle and then you could you have information if you recorded on video where the car ended up going so you can go back in time and predict what the car went based on the visual information that's very specific domain specific right but the question is whether we can come up with sort of a generic method for you know training machines to do this kind of prediction or filling in the blanks so right now uh this type of approach has been unbelievably successful in the context of natural language processing uh every modern natural language processing is pre-trained in self-supervised manner to fill in the blanks to you you show it a sequence of words you remove 10 percent of them and then you train some gigantic neural net to predict the words that are missing that and once you've pre-trained that network you can use the internal representation learn by it as input to you know something that you train supervised or whatever that's been incredibly successful not so successful in images although it's making progress and uh and it's based on uh sort of manual data augmentation we can go into this later but what has not been successful yet is training for video so getting a machine to learn to represent the visual world for example by just watching video nobody has really succeeded in doing this you

What is self-supervised learning, and why is it the "dark matter" of intelligence?

I'll start with the "dark matter" part. There is obviously a kind of learning that humans and animals are doing that we currently are not reproducing properly with machines and AI. The most popular approaches to machine learning today are supervised learning and reinforcement learning, and they are extremely inefficient.

Supervised learning requires many samples for learning anything, and reinforcement learning requires a ridiculously large number of trials and errors for a system to learn anything. This is why we don't have self-driving cars – it's a big leap from one to the other.

To solve difficult problems, you need a lot of human annotation for supervised learning to work, and to solve those difficult problems with reinforcement learning, you need a way to simulate the problem so that you can do the large-scale learning that reinforcement learning requires. How is it that most teenagers can learn to drive a car in about 20 hours of practice, whereas even with millions of hours of simulated practice, a self-driving car can't learn to drive itself properly? Clearly, we're missing something.

The immediate response from many people is that humans use their background knowledge to learn faster, and they're right. The question is, how was that background knowledge acquired? Babies in the first few months of life learn how the world works mostly by observation, because they can hardly act in the world. They learn an enormous amount of background knowledge about the world that may be the basis of what we call "common sense." This type of learning is not learning a task, and it's not being reinforced for anything – it's just observing the world and figuring out how it works, building world models.

**Self-supervised learning** is one attempt at reproducing this kind of learning. It's about just observing, not even the interacting part of a child – just sitting there, watching the world go by, having eyes open or closed, and the world appearing and disappearing. The idea is that this background knowledge, this understanding of the basic physics of objects, is what allows humans to learn to drive quickly – they've already observed cars operating in the world for many years before they start driving.

The mystery is how much signal, how much truth, is there in the world that a self-supervised system can learn from, without human assistance or with minimal human assistance. The trick is that there is way more signal in a self-supervised setting than there is in either supervised or reinforcement learning.

In supervised learning, you give the machine a few bits of information per sample, and in reinforcement learning, you give the machine a single scalar – a "good" or "bad" signal – only occasionally. But in self-supervised learning, the idea is to show the machine a video clip, stop it, and ask the machine to predict what's going to happen next. There's a huge amount of information in that – the entire video clip of the future after the initial clip you fed it.

The idea is that this formulation of filling in the gaps, predicting the future, or inferring the past, could be the key to solving intelligence for both language and vision. It's our best shot at the moment, whether it takes us all the way to human-level intelligence or just cat-level intelligence, it's worth exploring.
